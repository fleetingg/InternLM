## 1.什么是大模型

**一个模型能够解决跨模态的任务**

> 关于多模态大模型的发展，自从2021年openai发布了clip模型开始，就开始逐渐打破多模态的壁垒，实现了从图片到文本标签的预测，并且在zero-shot上的效果并不比当时的图片分类sota模型逊色很多，通过ImageNet微调过后的clip甚至能超过一些sota模型。这一种能够处理不同模态的数据，并且通过文本表达多样性，去进行zero-shot的任务部署，似乎越来越成为大模型的主流方向。
>
> zero-shot虽然是一个很有效的一个点，但是很多任务上，他的表现可能不如进行专门数据集微调过后的模型。为了对于特定任务有更好的效果，也可以进行一些参数微调或者是进行强化学习的策略（PPO）进行微调。失去模型一部分的思维广度，来换取在特定任务的更好的效果。



## 2.书生浦语

### 历史：

![image-20240329221055481](C:/Users/一曲流年/AppData/Roaming/Typora/typora-user-images/image-20240329221055481.png)

在发布的一年之中不断提高模型和生态链

**重点：开源！！！！！！！！！！！！**点一下close ai



### 模型大小

![image-20240329221222762](C:/Users/一曲流年/AppData/Roaming/Typora/typora-user-images/image-20240329221222762.png)

推理显存估算：7B-float 是 28 GB，7B-BF16 是 14GB，7B-int8 是 7GB
如果没有训练要求的话在推理的过程中用int8精度是没有太大问题，那么只需要一张显存大于7G的卡就可以进行模型部署了。
微调无脑A100



### 关于COT（Chain-of-Thought）

书生浦语会不会有一些类似于step by step的提示词来提高模型对于任务的理解呢



## 3.生态链

### 应用图

![image-20240329222935193](C:/Users/一曲流年/AppData/Roaming/Typora/typora-user-images/image-20240329222935193.png)

问题：全参数微调和部分参数微调之后的模型差距会很大吗？

![image-20240329223102256](C:/Users/一曲流年/AppData/Roaming/Typora/typora-user-images/image-20240329223102256.png)

一套很完全的体系，对于客户应用



### LORA（微调）

![image-20240329223853810](C:/Users/一曲流年/AppData/Roaming/Typora/typora-user-images/image-20240329223853810.png)

- 在原始 PLM (Pre-trained Language Model) 旁边增加一个旁路，做一个降维再升维的操作，来模拟所谓的`intrinsic rank`。

- 训练的时候固定 PLM 的参数，只训练降维矩阵 ![A](https://www.zhihu.com/equation?tex=A&consumer=ZHI_MENG) 与升维矩阵 ![B](https://www.zhihu.com/equation?tex=B&consumer=ZHI_MENG) 。而模型的输入输出维度不变，输出时将 ![BA](https://www.zhihu.com/equation?tex=BA&consumer=ZHI_MENG) 与 PLM 的参数叠加。

- 用随机高斯分布初始化 ![A](https://www.zhihu.com/equation?tex=A&consumer=ZHI_MENG) ，用 0 矩阵初始化 ![B](https://www.zhihu.com/equation?tex=B&consumer=ZHI_MENG) ，保证训练的开始此旁路矩阵依然是 0 矩阵

  Lora实际用：

![image-20240329223827163](C:/Users/一曲流年/AppData/Roaming/Typora/typora-user-images/image-20240329223827163.png)

### 模型评测方案openCompass

![image-20240329224056261](C:/Users/一曲流年/AppData/Roaming/Typora/typora-user-images/image-20240329224056261.png)

### 部署（KMDeploy）

![image-20240329224209596](C:/Users/一曲流年/AppData/Roaming/Typora/typora-user-images/image-20240329224209596.png)

有很好的接口，很好的GPU部署方案！

### 实用库（AgentLego）

![image-20240329224421540](C:/Users/一曲流年/AppData/Roaming/Typora/typora-user-images/image-20240329224421540.png)

支持openMMLab



## 4.总结

这是一套很全面的开发链条，可以尝试做一些多模态的人数，比之chatgpt4可能仍存在不足，但是这个模型应该是很支持中文语料库的，很适合国内用户的使用，并且**开源**（close ai）。

![image-20240329224530763](C:/Users/一曲流年/AppData/Roaming/Typora/typora-user-images/image-20240329224530763.png)
